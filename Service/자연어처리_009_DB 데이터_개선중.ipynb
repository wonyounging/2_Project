{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB 접속 정보 불러오기\n",
    "with open('c:/2nd_project/Service/DB_config.txt') as f:\n",
    "    conf = f.readlines()\n",
    "    DB_HOST = conf[0].replace('\\n','')\n",
    "    DB_USER = conf[1].replace('\\n','')\n",
    "    DB_PASSWORD = conf[2].replace('\\n','')\n",
    "    DB_NAME = conf[3].replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import logging\n",
    "\n",
    "class Database:\n",
    "    def __init__(self, host, user, password, db_name, charset='utf8'):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.db_name = db_name\n",
    "        self.charset = charset\n",
    "        self.conn = None\n",
    "\n",
    "    def connect(self):\n",
    "        if self.conn != None:\n",
    "            return\n",
    "        self.conn = pymysql.connect(\n",
    "            host=self.host,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            db=self.db_name,\n",
    "            charset=self.charset\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn is None:\n",
    "            return\n",
    "        if not self.conn.open:\n",
    "            self.conn = None\n",
    "            return\n",
    "        self.conn.close()\n",
    "        self.conn = None\n",
    "\n",
    "    def execute(self, sql):\n",
    "        last_row_id = -1\n",
    "        try:\n",
    "            with self.conn.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "            self.conn.commit()\n",
    "            last_row_id = cursor.lastrowid\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "        finally:\n",
    "            return last_row_id\n",
    "        \n",
    "    def select_one(self, sql):\n",
    "        result = None\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchone()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "        finally:\n",
    "            return result\n",
    "\n",
    "    def select_all(self, sql):\n",
    "        result = None\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchall()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "        finally:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_excel('./data/[KOBIS] 박스오피스_줄거리_감정키워드(2003.01~2023.07).xlsx', engine='openpyxl')\n",
    "# df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title, opendate, people, grade, genre, repnation, nations, Production, distributor, director, actors, story, keyword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert 쿼리 생성\n",
    "def InsertMovie(series):\n",
    "    try: \n",
    "        sql = f'''insert chat_movie(title, opendate, people, grade, genre, repnation, nations, \n",
    "        Production, distributor, director, actors, story, keyword) values (\"{series[0]}\", \"{series[1]}\",\n",
    "        \"{series[2]}\", \"{series[3]}\", \"{series[4]}\", \"{series[5]}\", \"{series[6]}\", \"{series[7]}\", \"{series[8]}\",\n",
    "        \"{series[9]}\", \"{series[10]}\", \"{series[11]}\", \"{series[12]}\")'''\n",
    "\n",
    "        # 엑셀에서 불러온 cell에 데이터가 없는 경우 null로 치환\n",
    "        sql = sql.replace('nan', 'null').replace('None','null')\n",
    "        \n",
    "        db.execute(sql)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        logging.error(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "# db.connect() \n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     InsertMovie(df.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "# db.connect() \n",
    "\n",
    "# db.select_all(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 답변 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "class FindAnswer:\n",
    "    def __init__(self, db, lsts):\n",
    "        self.db = db\n",
    "        self.lsts = lsts\n",
    "\n",
    "    def recent_day(self):\n",
    "        now = datetime.now()\n",
    "        six_months_ago = now - relativedelta(months=6)\n",
    "        recent = six_months_ago.strftime(\"%Y-%m-%d\")\n",
    "        return recent\n",
    "\n",
    "    def intent_query(self):\n",
    "        if self.lsts[0] == \"추천\":\n",
    "            sql_intent = ' order by rand() limit 1;'    # 랜덤으로 하나 추천\n",
    "        else:\n",
    "            sql_intent = ''\n",
    "        return sql_intent\n",
    "\n",
    "    def emotion_query(self):\n",
    "        if self.lsts[1] != \"없음\":\n",
    "            print('감정있음')\n",
    "            if self.lsts[2] != \"부정\":\n",
    "                print('그 감정 그대로')\n",
    "                sql_keyword = f\"keyword = '{self.lsts[1]}'\"\n",
    "            else:\n",
    "                print('그 감정 반대')\n",
    "                sql_keyword = f\"keyword != '{self.lsts[1]}'\"\n",
    "        else:\n",
    "            sql_keyword=''\n",
    "        return sql_keyword\n",
    "\n",
    "    def trend_query(self):\n",
    "        if self.lsts[3] == '최신':\n",
    "            sql_trend = f\"opendate >= '{self.recent_day()}'\" # 최신(6개월)\n",
    "        elif self.lsts[3] == '인기':\n",
    "            sql_trend = \"people >= 5000000\"\n",
    "        else:\n",
    "            sql_trend = \"people >= 1000000\"\n",
    "        return sql_trend\n",
    "\n",
    "    def ner_query(self):\n",
    "        sql_lst = []\n",
    "        lst = self.lsts[4]\n",
    "\n",
    "        # 배우 포함\n",
    "        if lst[1] != []:\n",
    "            for act in lst[1]:\n",
    "                sql = f\"actors like '%{act}%'\"\n",
    "                sql_lst.append(sql)\n",
    "                \n",
    "        # 장르 포함\n",
    "        if lst[2] != []:\n",
    "            for gen in lst[2]:\n",
    "                sql = f\"genre like '%{gen}%'\"\n",
    "                sql_lst.append(sql)\n",
    "        \n",
    "        # 국적 - 대표국적으로 구분\n",
    "        if lst[3] != []:\n",
    "            한국 = ['대한민국','우리나라','국내']\n",
    "            외국 = ['해외','외국']\n",
    "            for nat in lst[3]:\n",
    "                if nat in 한국:\n",
    "                    sql = \"repnations like '한국'\"\n",
    "                elif nat in 외국:\n",
    "                    sql = \"repnations not like '한국'\"\n",
    "                else:\n",
    "                    sql = f\"repnations like '{nat}'\"\n",
    "                sql_lst.append(sql)\n",
    "        \n",
    "        # 감독 포함\n",
    "        if lst[4] != []:\n",
    "            for direc in lst[4]:\n",
    "                sql = f\"director like '%{direc}%'\"\n",
    "                sql_lst.append(sql)\n",
    "                \n",
    "        if len(sql_lst)==0:\n",
    "            sql =''\n",
    "        else:\n",
    "            sql = ' and '.join(sql_lst)\n",
    "        return sql\n",
    "    \n",
    "    def final_query(self):\n",
    "        final_lst = []\n",
    "        final_lst.append(self.emotion_query())\n",
    "        final_lst.append(self.trend_query())\n",
    "        final_lst.append(self.ner_query())\n",
    "        try:\n",
    "            final_lst.remove('')\n",
    "        except:\n",
    "            pass\n",
    "        sql = ' and '.join(final_lst)\n",
    "        final_sql = \"select * from chat_movie where \" + sql + self.intent_query()\n",
    "        return final_sql\n",
    "    \n",
    "    def find_answer(self):\n",
    "        title = self.lsts[4][0]\n",
    "        time1 = self.lsts[4][5]\n",
    "        \n",
    "        if self.lsts[0] == '추천':\n",
    "            self.db.connect()\n",
    "            find_dict = self.db.select_all(self.final_query())\n",
    "            print(find_dict)\n",
    "            self.db.close()\n",
    "            \n",
    "            if find_dict == None:\n",
    "                ans = \"조건에 맞는 영화목록이 없습니다.\"\n",
    "                return ans\n",
    "            else:\n",
    "                ans = f'''영화 <{find_dict[0]['title']}> 추천드립니다.\\n\n",
    "                감독 <{find_dict[0]['director']}> \\n\n",
    "                감정 : <{find_dict[0]['keyword']}> '''\n",
    "                return ans\n",
    "            \n",
    "        elif self.lsts[0] == '후기':\n",
    "            ans = f'영화 <{title}> 후기입니다.'\n",
    "            return ans\n",
    "            \n",
    "        elif self.lsts[0] == '예매':\n",
    "            ans = f'영화 <{title}>가 <{time1}>에 예약되었습니다.'\n",
    "            return ans\n",
    "        \n",
    "        elif self.lsts[0] == '정보':\n",
    "            if title == []:\n",
    "                ans = \"해당 영화 정보가 없습니다.\"\n",
    "                return ans\n",
    "            \n",
    "            else:\n",
    "                self.db.connect()\n",
    "                find_dict = self.db.select_all(f'select * from chat_movie where title = \"{title[0]}\"')\n",
    "                self.db.close()\n",
    " \n",
    "                ans = f'영화 <{title}>의 정보입니다.\\n{find_dict[0]}'\n",
    "                return ans\n",
    "        else:\n",
    "            ans = '죄송합니다. 다시 이용해주세요.'\n",
    "            return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "import jpype\n",
    "\n",
    "class Preprocess :\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        # 단어 인덱스 사전 불러오기\n",
    "        if (word2index_dic != ''):\n",
    "            f = open(word2index_dic, 'rb')\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "            \n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "        \n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언, 기호, 어미, 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC',\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            'XSN', 'XSV', 'XSA'\n",
    "        ]\n",
    "        \n",
    "    # 형태소 분석기\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM()\n",
    "        return self.komoran.pos(sentence)\n",
    "        \n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    # 재밌는 영화, 무서운 영화, 액션 영화 등 NNP인 경우는 쪼개서 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_lst = []\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if p[1] == 'NNP':\n",
    "                if '영화' in p[0]:\n",
    "                    for q in p[0].split():\n",
    "                        word_lst.extend(self.pos(q))\n",
    "                else:\n",
    "                    word_lst.append(p)\n",
    "            else:\n",
    "                word_lst.append(p)\n",
    "                    \n",
    "        for word in word_lst:\n",
    "            if f(word[1]) is False:\n",
    "                word_list.append(word if without_tag is False else word[0])\n",
    "        return word_list\n",
    "    \n",
    "    # 키워드를 단어 인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "        \n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "        return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# 분류 모델 모듈\n",
    "class PredictModel:\n",
    "    def __init__(self, category, model_name, proprocess):\n",
    "        \n",
    "        self.MAX_SEQ_LEN = 15\n",
    "        self.category = category\n",
    "        \n",
    "        if self.category =='intent':\n",
    "            self.labels = {0: '기타', 1: '추천', 2: '후기', 3: '정보', 4: '예매', 5: '욕설'}\n",
    "            \n",
    "        elif self.category == 'emotion':\n",
    "            self.labels = {0: '무서움', 1: '슬픔', 2: '신남', 3: '없음', 4: '웃김', 5: '재미'}\n",
    "            \n",
    "        elif self.category == 'binary':\n",
    "            self.labels = {0: '긍정', 1: '부정', 2: '없음'}\n",
    "            \n",
    "        elif self.category == 'trend':\n",
    "            self.labels = {0: '없음', 1: '인기', 2: '최신'}\n",
    "        elif self.category == 'ner':\n",
    "            self.MAX_SEQ_LEN = 40\n",
    "            self.labels = {1: 'O', 2: 'B_MOVIE', 3: 'B_ACT', 4: 'B_GEN', 5: 'B_NAT', \n",
    "                           6: 'B_DIR', 7: 'B_DT', 8: 'B_RAT', 0: 'PAD'}  \n",
    "        else:\n",
    "            self.labels = {}\n",
    "\n",
    "        self.labels[len(self.labels)]=\"-\"\n",
    "        # 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = proprocess\n",
    "\n",
    "    # 클래스 예측\n",
    "    def predict_class(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=self.MAX_SEQ_LEN, padding='post')\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "\n",
    "        return predict_class.numpy()[0]\n",
    "\n",
    "        # ner_전용\n",
    "    def predict_ner(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0,\n",
    "                                                           maxlen=self.MAX_SEQ_LEN)\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "        tags = [self.labels[i] for i in predict_class.numpy()[0]]\n",
    "        return list(zip(keywords, tags))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# GPU에서 실행\n",
    "import tensorflow as tf\n",
    "with tf.device('/:GPU0'):\n",
    "        db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "        db.connect() \n",
    "\n",
    "        p = Preprocess(word2index_dic='c:/2nd_project/Data/chatbot_dict.bin',\n",
    "                        userdic = 'c:/2nd_project/Data/ner_data/new_user_dic_10.txt')\n",
    "\n",
    "        intent = PredictModel(category='intent', model_name='c:/2nd_project/Model/intent_model/intent_uso_model_0811_a_ep_best(5).h5', proprocess=p)\n",
    "        emotion = PredictModel(category='emotion', model_name='c:/2nd_project/Model/emotion_model/question_emotion_model.h5', proprocess=p)\n",
    "        binary = PredictModel(category='binary', model_name='c:/2nd_project/Model/binary_model/question_emotion_binary_model.h5', proprocess=p)\n",
    "        trend = PredictModel(category='trend', model_name='c:/2nd_project/Model/trend_model/question_trend_model.h5', proprocess=p)\n",
    "        ner = PredictModel(category='ner', model_name='c:/2nd_project/Model/ner_model/ner_model_0817_a.h5', proprocess=p)\n",
    "\n",
    "        def ner_tag_sep(lsts):  \n",
    "                ner_movie = []\n",
    "                ner_act = []\n",
    "                ner_gen = []\n",
    "                ner_nat = []\n",
    "                ner_dir = []\n",
    "                ner_dt = []\n",
    "                ner_rat = []\n",
    "\n",
    "                for lst in lsts:\n",
    "                        if lst[1] == 'B_MOVIE':\n",
    "                                ner_movie.append(lst[0])\n",
    "                        elif lst[1] == 'B_ACT':\n",
    "                                ner_act.append(lst[0])\n",
    "                        elif lst[1] == 'B_GEN':\n",
    "                                ner_gen.append(lst[0])\n",
    "                        elif lst[1] == 'B_NAT':\n",
    "                                ner_nat.append(lst[0])\n",
    "                        elif lst[1] == 'B_DIR':\n",
    "                                ner_dir.append(lst[0])\n",
    "                        elif lst[1] == 'B_DT':\n",
    "                                ner_dt.append(lst[0])\n",
    "                        elif lst[1] == 'B_RAT':\n",
    "                                ner_rat.append(lst[0])\n",
    "\n",
    "                        ner_movie = list(set(ner_movie))\n",
    "                        ner_act = list(set(ner_act))\n",
    "                        ner_gen = list(set(ner_gen))\n",
    "                        ner_nat = list(set(ner_nat))\n",
    "                        ner_dir = list(set(ner_dir))\n",
    "                        ner_dt = list(set(ner_dt))\n",
    "                        ner_rat = list(set(ner_rat))\n",
    "\n",
    "                return ner_movie, ner_act, ner_gen, ner_nat, ner_dir, ner_dt, ner_rat\n",
    "\n",
    "\n",
    "        def predict_keyword(text):\n",
    "        \n",
    "                intent_pred = intent.predict_class(text)\n",
    "                emotion_pred = emotion.predict_class(text)\n",
    "                binary_pred = binary.predict_class(text)\n",
    "                trend_pred = trend.predict_class(text)\n",
    "                ner_pred = ner.predict_ner(text)\n",
    "                \n",
    "                intent_label = intent.labels[intent_pred]\n",
    "                emotion_label = emotion.labels[emotion_pred]\n",
    "                binary_label = binary.labels[binary_pred]\n",
    "                trend_label = trend.labels[trend_pred]\n",
    "                ner_label = ner_tag_sep(ner_pred)\n",
    "\n",
    "                return intent_label, emotion_label, binary_label, trend_label, ner_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그냥 봉준호 감독 영화 추천\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjoeun\\AppData\\Local\\Temp\\ipykernel_5384\\1645926934.py:31: DeprecationWarning: jpype._core.attachThreadToJVM is deprecated, use java.lang.Thread.attach instead\n",
      "  jpype.attachThreadToJVM()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F695570D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 623ms/step\n",
      "('추천', '없음', '없음', '없음', ([], [], [], [], ['봉준호'], [], []))\n",
      "select * from chat_movie where people >= 1000000 and director like '%봉준호%' order by rand() limit 1;\n",
      "[{'title': '괴물', 'opendate': datetime.date(2006, 7, 27), 'people': '10917400', 'grade': '12세관람가', 'genre': 'SF,가족,드라마', 'repnation': '한국', 'nations': '한국', 'Production': '영화사청어람(주)', 'distributor': '(주)쇼박스', 'director': '봉준호', 'actors': '송강호,변희봉,박해일,배두나,고아성,오달수,박노식,라미란,고수희,윤제문,김뢰하,고창석,박진우,김학선,신현종,신승리,조덕제,조진영,김창렬,김병훈,이재응,이훈진,정강희,김대근,김효선,김종준,김다영,이왕우,유승목,손진호,권병길,손영순,정인기,최교식,이종윤,김비비,조영규', 'story': '한강 둔치에서 아버지(변희봉)의 매점 일을 돕고 있는 강두(송강호)는 모여 있는 사람들 속에서 무언가가 한강 다리에 매달려 움직이고 있는 것을 발견한다. 그리고 이내 정체를 알 수 없는 괴물이 둔치 위로 올라와 사람들을 거침없이 깔아뭉개고 무차별로 물어뜯기 시작한다. 아수라장이 된 한강변에서 강두는 뒤늦게 딸 현서(고아성)를 데리고 도망가다 경황 중에 현서의 손을 놓치고 만다. 그 순간 괴물은 현서를 낚아채 유유히 한강으로 사라진다. 한국 경찰과 군 당국, 그리고 미군은 바이러스 설을 운운하며 한강을 모두 폐쇄하고, 도시 전체는 마비된다. 강두는 바이러스 감염 여부를 검사하는 병실에서 죽은 줄 알았던 현서의 전화를 받고 그녀가 살아 있음을 알게 된다. 그러나 정부나 관계자들은 강두의 말을 믿지 않고, 바이러스에 감염된 상태에서 생긴 망상이라 치부한다. 결국 현직 양궁선수인 강두의 여동생 남주(배두나)와 전직 운동권 출신 남동생 남일(박해일)을 위시로 한 강두 가족은 바이러스 감염자를 모은 격리 시설에서 탈출하여 현서를 구하기 위해 직접 나선다.', 'keyword': 'null'}]\n",
      "영화 <괴물> 추천드립니다.\n",
      "\n",
      "                감독 <봉준호> \n",
      "\n",
      "                감정 : <null> \n"
     ]
    }
   ],
   "source": [
    "# text = '기생충 영화 드림 기생충 감독 봉준호 내가 좋아하는 배우는 황정민 박서준 고소영 우리나라 호러 장르는 액션  미국 오늘은 8월 전체관람가인 영화 추천 12시 안해주면 화날꺼같아'\n",
    "# query = '요즘 슬프지 않은 영화 볼만한거 없나'\n",
    "db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "\n",
    "query = '그냥 봉준호 감독 영화 추천'\n",
    "print(query)\n",
    "\n",
    "lsts = predict_keyword(query)\n",
    "\n",
    "# print('1. 의도 :', lsts[0])\n",
    "# print('2. 감정 :', lsts[1])\n",
    "# print('3. 긍부정 :', lsts[2])\n",
    "# print('4. 트렌드 :', lsts[3])\n",
    "# print('5. 개체명\\n\\t영화명 : {}\\n\\t배우 : {}\\n\\t장르 : {}\\n\\t국가 : {}\\n\\t감독 : {}\\n\\t시간 : {}\\n\\t등급 : {}'\n",
    "# .format(lsts[4][0], lsts[4][1], lsts[4][2], lsts[4][3], lsts[4][4], lsts[4][5], lsts[4][6]))\n",
    "print(lsts)\n",
    "\n",
    "obj = FindAnswer(db, lsts)\n",
    "sql = obj.final_query()\n",
    "print(sql)\n",
    "ans = obj.find_answer()\n",
    "print(ans)\n",
    "\n",
    "\n",
    "\n",
    "# print(sql)\n",
    "\n",
    "# movie_data = db.select_all(sql)\n",
    "# print(movie_data)\n",
    "\n",
    "# recomand_movie = movie_data[0]['title']\n",
    "# print(recomand_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화추천부탁드립니다\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:(1064, \"You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'order by rand() limit 1' at line 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('추천', '없음', '없음', '없음', ([], [], [], [], [], [], []))\n",
      "select * from chat_movie where people >= 1000000 and  order by rand() limit 1;\n",
      "None\n",
      "조건에 맞는 영화목록이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# text = '기생충 영화 드림 기생충 감독 봉준호 내가 좋아하는 배우는 황정민 박서준 고소영 우리나라 호러 장르는 액션  미국 오늘은 8월 전체관람가인 영화 추천 12시 안해주면 화날꺼같아'\n",
    "# query = '요즘 슬프지 않은 영화 볼만한거 없나'\n",
    "db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "\n",
    "query = '영화추천부탁드립니다'\n",
    "print(query)\n",
    "\n",
    "lsts = predict_keyword(query)\n",
    "\n",
    "# print('1. 의도 :', lsts[0])\n",
    "# print('2. 감정 :', lsts[1])\n",
    "# print('3. 긍부정 :', lsts[2])\n",
    "# print('4. 트렌드 :', lsts[3])\n",
    "# print('5. 개체명\\n\\t영화명 : {}\\n\\t배우 : {}\\n\\t장르 : {}\\n\\t국가 : {}\\n\\t감독 : {}\\n\\t시간 : {}\\n\\t등급 : {}'\n",
    "# .format(lsts[4][0], lsts[4][1], lsts[4][2], lsts[4][3], lsts[4][4], lsts[4][5], lsts[4][6]))\n",
    "print(lsts)\n",
    "\n",
    "obj = FindAnswer(db, lsts)\n",
    "sql = obj.final_query()\n",
    "print(sql)\n",
    "ans = obj.find_answer()\n",
    "print(ans)\n",
    "\n",
    "\n",
    "\n",
    "# print(sql)\n",
    "\n",
    "# movie_data = db.select_all(sql)\n",
    "# print(movie_data)\n",
    "\n",
    "# recomand_movie = movie_data[0]['title']\n",
    "# print(recomand_movie)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
