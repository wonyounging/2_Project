{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB 접속 정보 불러오기\n",
    "with open('c:/2nd_project/Service/DB_config.txt') as f:\n",
    "    conf = f.readlines()\n",
    "    DB_HOST = conf[0].replace('\\n','')\n",
    "    DB_USER = conf[1].replace('\\n','')\n",
    "    DB_PASSWORD = conf[2].replace('\\n','')\n",
    "    DB_NAME = conf[3].replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import logging\n",
    "\n",
    "class Database:\n",
    "    def __init__(self, host, user, password, db_name, charset='utf8'):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.db_name = db_name\n",
    "        self.charset = charset\n",
    "        self.conn = None\n",
    "\n",
    "    def connect(self):\n",
    "        if self.conn != None:\n",
    "            return\n",
    "        self.conn = pymysql.connect(\n",
    "            host=self.host,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            db=self.db_name,\n",
    "            charset=self.charset\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn is None:\n",
    "            return\n",
    "        if not self.conn.open:\n",
    "            self.conn = None\n",
    "            return\n",
    "        self.conn.close()\n",
    "        self.conn = None\n",
    "\n",
    "    def execute(self, sql):\n",
    "        last_row_id = -1\n",
    "        try:\n",
    "            with self.conn.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "            self.conn.commit()\n",
    "            last_row_id = cursor.lastrowid\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "        finally:\n",
    "            return last_row_id\n",
    "        \n",
    "    def select_one(self, sql):\n",
    "        result = None\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchone()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "        finally:\n",
    "            return result\n",
    "\n",
    "    def select_all(self, sql):\n",
    "        result = None\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchall()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "        finally:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "영화명                                                  #살아있다\n",
       "개봉일                                             2020-06-24\n",
       "누적관객수                                              1903992\n",
       "등급                                                15세이상관람가\n",
       "장르                                                     드라마\n",
       "대표국적                                                    한국\n",
       "국적                                                      한국\n",
       "제작사                                      영화사 집,(주)퍼스펙티브픽쳐스\n",
       "배급사                                      롯데컬처웍스(주)롯데엔터테인먼트\n",
       "감독                                                     조일형\n",
       "배우       유아인,박신혜,전배수,고나영,권용채,김경태,김다영,김단비,김라희,김미래,김미래,김윤...\n",
       "줄거리                                                    NaN\n",
       "키워드                                                    NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_excel('c:/2nd_project/Data/movie_data/[KOBIS] 박스오피스_줄거리_감정키워드(2003.01~2023.07).xlsx', engine='openpyxl')\n",
    "# df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title, opendate, people, grade, genre, repnation, nations, Production, distributor, director, actors, story, keyword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert 쿼리 생성\n",
    "def InsertMovie(series):\n",
    "    try: \n",
    "        sql = f'''insert chat_movie(title, opendate, people, grade, genre, repnation, nations, \n",
    "        Production, distributor, director, actors, story, keyword) values (\"{series[0]}\", \"{series[1]}\",\n",
    "        \"{series[2]}\", \"{series[3]}\", \"{series[4]}\", \"{series[5]}\", \"{series[6]}\", \"{series[7]}\", \"{series[8]}\",\n",
    "        \"{series[9]}\", \"{series[10]}\", \"{series[11]}\", \"{series[12]}\")'''\n",
    "\n",
    "        # 엑셀에서 불러온 cell에 데이터가 없는 경우 null로 치환\n",
    "        sql = sql.replace('nan', 'null').replace('None','null')\n",
    "        \n",
    "        db.execute(sql)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        logging.error(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "# db.connect() \n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     InsertMovie(df.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "# db.connect() \n",
    "\n",
    "# db.select_all(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 답변 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "class FindAnswer:\n",
    "    def __init__(self, db, lsts):\n",
    "        self.db = db\n",
    "        self.lsts = lsts\n",
    "\n",
    "    def recent_day(self):\n",
    "        now = datetime.now()\n",
    "        six_months_ago = now - relativedelta(months=6)\n",
    "        recent = six_months_ago.strftime(\"%Y-%m-%d\")\n",
    "        return recent\n",
    "\n",
    "    def time_sort(self,times):\n",
    "        sort_time = [[],[],[],[],[]]\n",
    "        return_time = []\n",
    "    \n",
    "        for time in times:\n",
    "            if \"월\" in time or \"화\" in time or \"수\" in time or \"목\" in time or \"금\" in time or \"토\" in time or \"일\" in time:\n",
    "                if \"요일\" in time:\n",
    "                    sort_time[4] = time\n",
    "                elif \"일\" in time:\n",
    "                    sort_time[1] = time\n",
    "                else:\n",
    "                    sort_time[0] = time\n",
    "            elif \"시\" in time:\n",
    "                sort_time[2] = time\n",
    "            elif \"분\" in time:\n",
    "                sort_time[3] = time\n",
    "            \n",
    "        for st in sort_time:\n",
    "            if len(st) > 0:\n",
    "                return_time.append(st)\n",
    "        return return_time\n",
    "\n",
    "    def intent_query(self):\n",
    "        if self.lsts[0] == \"추천\":\n",
    "            sql_intent = ' order by rand() limit 1;'    # 랜덤으로 하나 추천\n",
    "        else:\n",
    "            sql_intent = ''\n",
    "        return sql_intent\n",
    "\n",
    "    def emotion_query(self):\n",
    "        if self.lsts[1] != \"없음\":\n",
    "            if self.lsts[2] != \"부정\":\n",
    "                sql_keyword = f\"keyword = '{self.lsts[1]}'\"\n",
    "            else:\n",
    "                sql_keyword = f\"keyword != '{self.lsts[1]}'\"\n",
    "        else:\n",
    "            sql_keyword=''\n",
    "        return sql_keyword\n",
    "\n",
    "    def trend_query(self):\n",
    "        if self.lsts[3] == '최신':\n",
    "            sql_trend = f\"opendate >= '{self.recent_day()}'\" # 최신(6개월)\n",
    "        elif self.lsts[3] == '인기':\n",
    "            sql_trend = \"people >= 5000000\"\n",
    "        else:\n",
    "            sql_trend = \"people >= 1000000\"\n",
    "        return sql_trend\n",
    "\n",
    "    def ner_query(self):\n",
    "        sql_lst = []\n",
    "        lst = self.lsts[4]\n",
    "\n",
    "        # 배우 포함\n",
    "        if lst[1] != []:\n",
    "            for act in lst[1]:\n",
    "                sql = f\"actors like '%{act}%'\"\n",
    "                sql_lst.append(sql)\n",
    "                \n",
    "        # 장르 포함\n",
    "        if lst[2] != []:\n",
    "            for gen in lst[2]:\n",
    "                sql = f\"genre like '%{gen}%'\"\n",
    "                sql_lst.append(sql)\n",
    "        \n",
    "        # 국적 - 대표국적으로 구분\n",
    "        if lst[3] != []:\n",
    "            한국 = ['대한민국','우리나라','국내']\n",
    "            외국 = ['해외','외국']\n",
    "            for nat in lst[3]:\n",
    "                if nat in 한국:\n",
    "                    sql = \"repnations like '한국'\"\n",
    "                elif nat in 외국:\n",
    "                    sql = \"repnations not like '한국'\"\n",
    "                else:\n",
    "                    sql = f\"repnations like '{nat}'\"\n",
    "                sql_lst.append(sql)\n",
    "        \n",
    "        # 감독 포함\n",
    "        if lst[4] != []:\n",
    "            for direc in lst[4]:\n",
    "                sql = f\"director like '%{direc}%'\"\n",
    "                sql_lst.append(sql)\n",
    "                \n",
    "        if len(sql_lst)==0:\n",
    "            sql =''\n",
    "        else:\n",
    "            sql = ' and '.join(sql_lst)\n",
    "        return sql\n",
    "    \n",
    "    def final_query(self):\n",
    "        final_lst = []\n",
    "        final_lst.append(self.emotion_query())\n",
    "        final_lst.append(self.trend_query())\n",
    "        final_lst.append(self.ner_query())\n",
    "        # try:\n",
    "        #     final_lst.remove('')\n",
    "        # except:\n",
    "        #     pass\n",
    "        final_lst = [item for item in final_lst if item]\n",
    "                \n",
    "        sql = ' and '.join(final_lst)\n",
    "        final_sql = \"select * from chat_movie where \" + sql + self.intent_query()\n",
    "        return final_sql\n",
    "    \n",
    "    def find_answer(self):\n",
    "        title = self.lsts[4][0]\n",
    "        time = self.time_sort(self.lsts[4][5])\n",
    "        \n",
    "        \n",
    "        if self.lsts[0] == '추천':\n",
    "            self.db.connect()\n",
    "            find_dict = self.db.select_all(self.final_query())\n",
    "            print(find_dict)\n",
    "            self.db.close()\n",
    "            \n",
    "            if len(find_dict) == 0 :\n",
    "                ans = \"조건에 맞는 영화목록이 없습니다.\"\n",
    "                return ans\n",
    "            else:\n",
    "                ans = f'''영화 {find_dict[0]['title']} 추천드립니다.\\n{'='*50}\\n등급 : {find_dict[0]['grade']}\\n장르 : {find_dict[0]['genre']}\\n감독 : {find_dict[0]['director']}\\n배우 : {' / '.join(find_dict[0]['actors'].split(',')[:3])}\\n개봉일 : {find_dict[0]['opendate']}'''\n",
    "                return ans\n",
    "            \n",
    "        elif self.lsts[0] == '후기':\n",
    "            ans = f'''영화 {title} 후기입니다.\\n{'='*50}\\n'''\n",
    "            return ans\n",
    "            \n",
    "        elif self.lsts[0] == '예매':\n",
    "            if len(self.lsts[4][0]) > 0 and len(self.lsts[4][5]) > 0:\n",
    "                ans = f'''영화 {title[0]}이/가 {' '.join(time)}에 예약되었습니다.'''\n",
    "            else:\n",
    "                ans = f\"예매하고 싶은 영화명과 예매 시간을 포함해서 다시 문의주세요.\"\n",
    "            return ans\n",
    "        \n",
    "        elif self.lsts[0] == '정보':\n",
    "            if title == []:\n",
    "                ans = \"해당 영화 정보가 없습니다.\"\n",
    "                return ans\n",
    "            \n",
    "            else:\n",
    "                self.db.connect()\n",
    "                find_dict = self.db.select_all(f'select * from chat_movie where title = \"{title[0]}\"')\n",
    "                self.db.close()\n",
    " \n",
    "                ans = f'''영화 {title}의 정보입니다.\\n{'='*50}\\n등급 : {find_dict[0]['grade']}\\n장르 : {find_dict[0]['genre']}\\n제작국가 : {find_dict[0]['nations']}\\n감독 : {find_dict[0]['director']}\\n배우 : {' / '.join(find_dict[0]['actors'].split(',')[:3])}\\n개봉일 : {find_dict[0]['opendate']}\\n\\n줄거리 : \\n{find_dict[0]['story']}'''\n",
    "                return ans\n",
    "        else:\n",
    "            ans = '죄송합니다. 다시 이용해주세요.'\n",
    "            return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "import jpype\n",
    "\n",
    "class Preprocess :\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        # 단어 인덱스 사전 불러오기\n",
    "        if (word2index_dic != ''):\n",
    "            f = open(word2index_dic, 'rb')\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "            \n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "        \n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언, 기호, 어미, 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC',\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            'XSN', 'XSV', 'XSA'\n",
    "        ]\n",
    "        \n",
    "    # 형태소 분석기\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM()\n",
    "        return self.komoran.pos(sentence)\n",
    "        \n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    # 재밌는 영화, 무서운 영화, 액션 영화 등 NNP인 경우는 쪼개서 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_lst = []\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if p[1] == 'NNP':\n",
    "                if '영화' in p[0]:\n",
    "                    for q in p[0].split():\n",
    "                        word_lst.extend(self.pos(q))\n",
    "                else:\n",
    "                    word_lst.append(p)\n",
    "            else:\n",
    "                word_lst.append(p)\n",
    "                    \n",
    "        for word in word_lst:\n",
    "            if f(word[1]) is False:\n",
    "                word_list.append(word if without_tag is False else word[0])\n",
    "        return word_list\n",
    "    \n",
    "    # 키워드를 단어 인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "        \n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "        return w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# 분류 모델 모듈\n",
    "class PredictModel:\n",
    "    def __init__(self, category, model_name, proprocess):\n",
    "        \n",
    "        self.MAX_SEQ_LEN = 15\n",
    "        self.category = category\n",
    "        \n",
    "        if self.category =='intent':\n",
    "            self.labels = {0: '기타', 1: '추천', 2: '후기', 3: '정보', 4: '예매', 5: '욕설'}\n",
    "            \n",
    "        elif self.category == 'emotion':\n",
    "            self.labels = {0: '무서움', 1: '슬픔', 2: '신남', 3: '없음', 4: '웃김', 5: '재미'}\n",
    "            \n",
    "        elif self.category == 'binary':\n",
    "            self.labels = {0: '긍정', 1: '부정', 2: '없음'}\n",
    "            \n",
    "        elif self.category == 'trend':\n",
    "            self.labels = {0: '없음', 1: '인기', 2: '최신'}\n",
    "        elif self.category == 'ner':\n",
    "            self.MAX_SEQ_LEN = 40\n",
    "            self.labels = {1: 'O', 2: 'B_MOVIE', 3: 'B_ACT', 4: 'B_GEN', 5: 'B_NAT', \n",
    "                           6: 'B_DIR', 7: 'B_DT', 8: 'B_RAT', 0: 'PAD'}  \n",
    "        else:\n",
    "            self.labels = {}\n",
    "\n",
    "        self.labels[len(self.labels)]=\"-\"\n",
    "        # 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = proprocess\n",
    "\n",
    "    # 클래스 예측\n",
    "    def predict_class(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=self.MAX_SEQ_LEN, padding='post')\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "\n",
    "        return predict_class.numpy()[0]\n",
    "\n",
    "        # ner_전용\n",
    "    def predict_ner(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0,\n",
    "                                                           maxlen=self.MAX_SEQ_LEN)\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "        tags = [self.labels[i] for i in predict_class.numpy()[0]]\n",
    "        return list(zip(keywords, tags))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# GPU에서 실행\n",
    "import tensorflow as tf\n",
    "with tf.device('/:GPU0'):\n",
    "        db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "        db.connect() \n",
    "\n",
    "        p = Preprocess(word2index_dic='c:/2nd_project/Data/chatbot_dict.bin',\n",
    "                        userdic = 'c:/2nd_project/Data/ner_data/new_user_dic_10.txt')\n",
    "\n",
    "        intent = PredictModel(category='intent', model_name='c:/2nd_project/Model/intent_model/intent_uso_model_0811_a_ep_best(5).h5', proprocess=p)\n",
    "        emotion = PredictModel(category='emotion', model_name='c:/2nd_project/Model/emotion_model/question_emotion_model.h5', proprocess=p)\n",
    "        binary = PredictModel(category='binary', model_name='c:/2nd_project/Model/binary_model/question_emotion_binary_model.h5', proprocess=p)\n",
    "        trend = PredictModel(category='trend', model_name='c:/2nd_project/Model/trend_model/question_trend_model.h5', proprocess=p)\n",
    "        ner = PredictModel(category='ner', model_name='c:/2nd_project/Model/ner_model/ner_model_0817_a.h5', proprocess=p)\n",
    "\n",
    "        def ner_tag_sep(lsts):  \n",
    "                ner_movie = []\n",
    "                ner_act = []\n",
    "                ner_gen = []\n",
    "                ner_nat = []\n",
    "                ner_dir = []\n",
    "                ner_dt = []\n",
    "                ner_rat = []\n",
    "\n",
    "                for lst in lsts:\n",
    "                        if lst[1] == 'B_MOVIE':\n",
    "                                ner_movie.append(lst[0])\n",
    "                        elif lst[1] == 'B_ACT':\n",
    "                                ner_act.append(lst[0])\n",
    "                        elif lst[1] == 'B_GEN':\n",
    "                                ner_gen.append(lst[0])\n",
    "                        elif lst[1] == 'B_NAT':\n",
    "                                ner_nat.append(lst[0])\n",
    "                        elif lst[1] == 'B_DIR':\n",
    "                                ner_dir.append(lst[0])\n",
    "                        elif lst[1] == 'B_DT':\n",
    "                                ner_dt.append(lst[0])\n",
    "                        elif lst[1] == 'B_RAT':\n",
    "                                ner_rat.append(lst[0])\n",
    "\n",
    "                        ner_movie = list(set(ner_movie))\n",
    "                        ner_act = list(set(ner_act))\n",
    "                        ner_gen = list(set(ner_gen))\n",
    "                        ner_nat = list(set(ner_nat))\n",
    "                        ner_dir = list(set(ner_dir))\n",
    "                        ner_dt = list(set(ner_dt))\n",
    "                        ner_rat = list(set(ner_rat))\n",
    "\n",
    "                return ner_movie, ner_act, ner_gen, ner_nat, ner_dir, ner_dt, ner_rat\n",
    "\n",
    "\n",
    "        def predict_keyword(text):\n",
    "        \n",
    "                intent_pred = intent.predict_class(text)\n",
    "                emotion_pred = emotion.predict_class(text)\n",
    "                binary_pred = binary.predict_class(text)\n",
    "                trend_pred = trend.predict_class(text)\n",
    "                ner_pred = ner.predict_ner(text)\n",
    "                \n",
    "                intent_label = intent.labels[intent_pred]\n",
    "                emotion_label = emotion.labels[emotion_pred]\n",
    "                binary_label = binary.labels[binary_pred]\n",
    "                trend_label = trend.labels[trend_pred]\n",
    "                ner_label = ner_tag_sep(ner_pred)\n",
    "\n",
    "                return intent_label, emotion_label, binary_label, trend_label, ner_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # text = '기생충 영화 드림 기생충 감독 봉준호 내가 좋아하는 배우는 황정민 박서준 고소영 우리나라 호러 장르는 액션  미국 오늘은 8월 전체관람가인 영화 추천 12시 안해주면 화날꺼같아'\n",
    "# # query = '요즘 슬프지 않은 영화 볼만한거 없나'\n",
    "# db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "\n",
    "# # query = '그냥 봉준호 감독 영화 정보좀'\n",
    "# # query = '겨울왕국 정보좀'\n",
    "# # query = '아무영화 추천좀'\n",
    "# # query = '명량 다음주 금요일에 예약해줘'\n",
    "# # query = '명량 이번주 화요일 13일에 예약해줘'\n",
    "# # query = '명량 목요일 20시 30분에 예약해줘'\n",
    "# # query = '명량 1월 15일 12시에 예약해줘'\n",
    "# # query = '명량 예약해줘'\n",
    "# query = '8시 예약해줘'\n",
    "# # query = '명량 30분에 예약해줘'\n",
    "\n",
    "# # query = '재밌는 한국 영화 추천좀'\n",
    "\n",
    "# print(query)\n",
    "\n",
    "# lsts = predict_keyword(query)\n",
    "\n",
    "# # print('1. 의도 :', lsts[0])\n",
    "# # print('2. 감정 :', lsts[1])\n",
    "# # print('3. 긍부정 :', lsts[2])\n",
    "# # print('4. 트렌드 :', lsts[3])\n",
    "# # print('5. 개체명\\n\\t영화명 : {}\\n\\t배우 : {}\\n\\t장르 : {}\\n\\t국가 : {}\\n\\t감독 : {}\\n\\t시간 : {}\\n\\t등급 : {}'\n",
    "# # .format(lsts[4][0], lsts[4][1], lsts[4][2], lsts[4][3], lsts[4][4], lsts[4][5], lsts[4][6]))\n",
    "# print(lsts)\n",
    "\n",
    "# obj = FindAnswer(db, lsts)\n",
    "# sql = obj.final_query()\n",
    "# # print(sql)\n",
    "# ans = obj.find_answer()\n",
    "# print(ans)\n",
    "\n",
    "\n",
    "\n",
    "# # print(sql)\n",
    "\n",
    "# # movie_data = db.select_all(sql)\n",
    "# # print(movie_data)\n",
    "\n",
    "# # recomand_movie = movie_data[0]['title']\n",
    "# # print(recomand_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아이언맨 정보 알려줘\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "('추천', '없음', '없음', '없음', (['아이언맨'], [], [], [], [], [], []))\n",
      "select * from chat_movie where people >= 1000000 order by rand() limit 1;\n",
      "[{'title': '조선명탐정 : 각시투구꽃의 비밀', 'opendate': datetime.date(2011, 1, 27), 'people': '4786259', 'grade': '12세이상관람가', 'genre': '코미디,액션', 'repnations': '한국', 'nations': '한국', 'Production': '청년필름(주)', 'distributor': '(주)쇼박스', 'director': '김석윤', 'actors': '김명민,김호연,한지민,오달수,김영훈,백성기,지승현,류현상,김경화,김탁호,김윤섭,최무성,남성진,우현,이재용,예수정,원현준,김태훈,정인기', 'story': '정조 16년, 조선을 뒤흔들 거대한 스캔들… 조선 제일 명탐정이 나가신다!정조 16년, 의문의 연쇄 살인사건이 공납 비리를 숨기려는 관료들의 음모일 거라 짐작한 정조는 명탐정(김명민)에게 사건의 배후를 찾으라는 밀명을 내린다. 수사 첫날부터 자객의 습격을 받은 명탐정은 개장수 서필(오달수)의 도움으로 위기를 모면하게 되고, 서필과 함께 사건의 결정적 단서인 각시투구꽃을 찾아 적성으로 향하게 된다. 그 곳에서 명탐정과 서필은 조선의 상단을 주름잡으며 사건의 열쇠를 쥐고 있는 한객주(한지민)를 만나게 되는데…', 'keyword': '웃김'}]\n",
      "영화 조선명탐정 : 각시투구꽃의 비밀 추천드립니다.\n",
      "==================================================\n",
      "등급 : 12세이상관람가\n",
      "장르 : 코미디,액션\n",
      "감독 : 김석윤\n",
      "배우 : 김명민 / 김호연 / 한지민\n",
      "개봉일 : 2011-01-27\n"
     ]
    }
   ],
   "source": [
    "db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "\n",
    "query = '아이언맨 정보 알려줘'\n",
    "print(query)\n",
    "\n",
    "lsts = predict_keyword(query)\n",
    "print(lsts)\n",
    "\n",
    "obj = FindAnswer(db, lsts)\n",
    "\n",
    "sql = obj.final_query()\n",
    "print(sql)\n",
    "\n",
    "ans = obj.find_answer()\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
