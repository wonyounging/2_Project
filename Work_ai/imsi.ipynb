{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 임포트\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>그거 말고 추천작은요</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>웹툰 추천 좀 해 보이세</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>신과 함께 추천이여</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  label\n",
       "43     그거 말고 추천작은요      1\n",
       "726  웹툰 추천 좀 해 보이세      1\n",
       "731     신과 함께 추천이여      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.read_csv('d:/2nd_project/Data/talk_data/영화주제 대화 말뭉치 라벨링.csv')\n",
    "corpus_df = corpus_df[corpus_df['label'] != 0]\n",
    "corpus_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"d:/2nd_project/Data/talk_data/영화주제 대화 말뭉치 라벨링.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "data = data[data['label'] != 0]\n",
    "queries = data['text'].tolist()\n",
    "intents = data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess2 import Preprocess2\n",
    "p = Preprocess2(word2index_dic='d:/2nd_project/Data/chatbot_dict_talk.bin',\n",
    "               userdic = 'd:/2nd_project/Data/user_dic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for sentence in queries:\n",
    "    sentence = str(sentence)\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  37,   68,  136, ...,    0,    0,    0],\n",
       "       [  90,  136,   94, ...,    0,    0,    0],\n",
       "       [ 445,  684,  136, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 185,   28,  317, ...,    0,    0,    0],\n",
       "       [ 185, 3471,  317, ...,    0,    0,    0],\n",
       "       [1173,   28,   85, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = corpus_df[['text']].astype(str)\n",
    "X = padded_seqs\n",
    "y = corpus_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_label = encoder.fit_transform(y)\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_label))\n",
    "np.unique(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(padded_seqs, intents, stratify=intents, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_seqs, y_label, stratify=intents, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y_label, stratify=y_label, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    1272\n",
       "0     608\n",
       "3     191\n",
       "1     150\n",
       "2      19\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1 # 전체 단어수 (패딩 0 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(20)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(None, 15), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 15)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 15, 128)              1923968   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 15, 128)              0         ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 15, 128)              49280     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 15, 128)              65664     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 15, 128)              82048     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " global_max_pooling1d (Glob  (None, 128)                  0         ['conv1d[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Gl  (None, 128)                  0         ['conv1d_1[0][0]']            \n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Gl  (None, 128)                  0         ['conv1d_2[0][0]']            \n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 384)                  0         ['global_max_pooling1d[0][0]',\n",
      "                                                                     'global_max_pooling1d_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'global_max_pooling1d_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  49280     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 128)                  16512     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 128)                  16512     ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 128)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " logits (Dense)              (None, 5)                    645       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 5)                    30        ['logits[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2203939 (8.41 MB)\n",
      "Trainable params: 2203939 (8.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print(\"*\"*50)\n",
    "# print(padded_seqs)\n",
    "# print(padded_seqs.shape)\n",
    "# print('*'*50)\n",
    "# CNN 모델 정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate = dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='same',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=4,\n",
    "    padding='same',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='same',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 3,4,5gram 이후 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden1 = Dense(128, activation=tf.nn.relu)(concat)\n",
    "hidden2 = Dense(128, activation=tf.nn.relu)(hidden1)\n",
    "hidden3 = Dense(128, activation=tf.nn.relu)(hidden2)\n",
    "\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden3)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "112/112 [==============================] - 11s 66ms/step - loss: 0.6119 - accuracy: 0.7888 - val_loss: 0.1948 - val_accuracy: 0.9643\n",
      "Epoch 2/5\n",
      "112/112 [==============================] - 7s 63ms/step - loss: 0.1270 - accuracy: 0.9643 - val_loss: 0.0877 - val_accuracy: 0.9804\n",
      "Epoch 3/5\n",
      "112/112 [==============================] - 7s 64ms/step - loss: 0.0538 - accuracy: 0.9844 - val_loss: 0.0871 - val_accuracy: 0.9839\n",
      "Epoch 4/5\n",
      "112/112 [==============================] - 7s 60ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.1023 - val_accuracy: 0.9804\n",
      "Epoch 5/5\n",
      "112/112 [==============================] - 7s 64ms/step - loss: 0.0116 - accuracy: 0.9960 - val_loss: 0.0689 - val_accuracy: 0.9893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python38\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "model.save('d:/2nd_project/Model/intent_model_0808_b.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 의도 분류 모델 모듈\n",
    "class StoryModel:\n",
    "    def __init__(self, model_name, proprocess):\n",
    "        # intent 레이블\n",
    "        # self.labels = dict(zip(range(0,12),encoder.classes_.tolist()))\n",
    "        self.labels = {0: '추천',\n",
    "                        1: '후기',\n",
    "                        2: '정보',\n",
    "                        3: '예매',\n",
    "                        4: '욕설',\n",
    "                        }\n",
    "        # intent 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = proprocess\n",
    "\n",
    "    # 의도 클래스 예측\n",
    "    def predict_class(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "\n",
    "        return predict_class.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 210ms/step\n",
      "오늘 영화 예약 해줘\n",
      "의도 예측 클래스 :  3\n",
      "의도 예측 레이블 :  예매\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "씨발\n",
      "의도 예측 클래스 :  4\n",
      "의도 예측 레이블 :  욕설\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "내일 영화 추천 해봐라\n",
      "의도 예측 클래스 :  0\n",
      "의도 예측 레이블 :  추천\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "인터스텔라 내용이 뭐야?\n",
      "의도 예측 클래스 :  2\n",
      "의도 예측 레이블 :  정보\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "7번방의 선물 후기 좀\n",
      "의도 예측 클래스 :  1\n",
      "의도 예측 레이블 :  후기\n"
     ]
    }
   ],
   "source": [
    "from Preprocess2 import Preprocess2\n",
    "p = Preprocess2(word2index_dic='d:/2nd_project/Data/chatbot_dict_talk.bin',\n",
    "               userdic = 'd:/2nd_project/Data/user_dic.txt')\n",
    "\n",
    "intent = StoryModel(model_name='d:/2nd_project/Model/intent_model_0808_b.h5', proprocess=p)\n",
    "\n",
    "items=[\"오늘 영화 예약 해줘\", \"씨발\", \"내일 영화 추천 해봐라\", \"인터스텔라 내용이 뭐야?\", \"7번방의 선물 후기 좀\"]\n",
    "\n",
    "for item in items:\n",
    "\n",
    "    predict = intent.predict_class(item)\n",
    "\n",
    "    predict_label = intent.labels[predict]\n",
    "\n",
    "    print(item)\n",
    "\n",
    "    print(\"의도 예측 클래스 : \", predict)\n",
    "\n",
    "    print(\"의도 예측 레이블 : \", predict_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
